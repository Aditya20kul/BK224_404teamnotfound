{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SIH ANN",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dlziSRgygXW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f6f00144-4932-4c29-f20e-c0c65a2c8bde"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KG2jc3hzCSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models  import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint \n",
        "import pandas as pd"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzwlQXNizqe-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "944e88d9-f01b-42af-ca1e-aa23fadbfef0"
      },
      "source": [
        "%cd drive/My\\ Drive/SIH"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/SIH\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbhY0XvNzxic",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "974a2f9d-5bc3-4999-a2d4-ea13ececc150"
      },
      "source": [
        "df= pd.read_excel('ANN_sih_Data.xls')\n",
        "df.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI9_DqxI0FFb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0aef9039-67ed-49ac-cba2-a091f129777e"
      },
      "source": [
        "train=df[['Thi','Tho','Tco','Tci','U','Q_Ideal','LMTD']]\n",
        "y=df[['Efficiency']]\n",
        "print(train.shape[1])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S342v4SqytY_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "aaac8f09-eda6-4634-f93d-eec443fd124f"
      },
      "source": [
        "NN_model = Sequential()\n",
        "\n",
        "# The Input Layer :\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',input_dim=7, activation='relu'))\n",
        "\n",
        "# The Hidden Layers :\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(128, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(64, kernel_initializer='normal',activation='relu'))\n",
        "\n",
        "# The Output Layer :\n",
        "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
        "\n",
        "# Compile the network :\n",
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "NN_model.summary()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_70 (Dense)             (None, 256)               2048      \n",
            "_________________________________________________________________\n",
            "dense_71 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_72 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_73 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_74 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_75 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 174,849\n",
            "Trainable params: 174,849\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnrGaOAM0J3h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7252f2dc-0701-42ab-f309-00c02c09f632"
      },
      "source": [
        "checkpoint_name = 'Train 2 Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "callbacks_list = [checkpoint]\n",
        "history = NN_model.fit(train,y,epochs=200, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 32000 samples, validate on 8000 samples\n",
            "Epoch 1/200\n",
            "32000/32000 [==============================] - 4s 137us/step - loss: 20.4403 - mean_absolute_error: 20.4403 - accuracy: 0.0000e+00 - val_loss: 19.8962 - val_mean_absolute_error: 19.8962 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 19.89624, saving model to Train 2 Weights-001--19.89624.hdf5\n",
            "Epoch 2/200\n",
            "32000/32000 [==============================] - 4s 133us/step - loss: 20.0265 - mean_absolute_error: 20.0265 - accuracy: 0.0000e+00 - val_loss: 20.0272 - val_mean_absolute_error: 20.0272 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 19.89624\n",
            "Epoch 3/200\n",
            "32000/32000 [==============================] - 4s 135us/step - loss: 19.9318 - mean_absolute_error: 19.9317 - accuracy: 0.0000e+00 - val_loss: 19.7796 - val_mean_absolute_error: 19.7796 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00003: val_loss improved from 19.89624 to 19.77960, saving model to Train 2 Weights-003--19.77960.hdf5\n",
            "Epoch 4/200\n",
            "32000/32000 [==============================] - 4s 132us/step - loss: 19.8592 - mean_absolute_error: 19.8592 - accuracy: 0.0000e+00 - val_loss: 19.7704 - val_mean_absolute_error: 19.7704 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00004: val_loss improved from 19.77960 to 19.77042, saving model to Train 2 Weights-004--19.77042.hdf5\n",
            "Epoch 5/200\n",
            "32000/32000 [==============================] - 4s 132us/step - loss: 19.8148 - mean_absolute_error: 19.8148 - accuracy: 0.0000e+00 - val_loss: 20.1111 - val_mean_absolute_error: 20.1111 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 19.77042\n",
            "Epoch 6/200\n",
            "32000/32000 [==============================] - 4s 131us/step - loss: 19.8466 - mean_absolute_error: 19.8466 - accuracy: 0.0000e+00 - val_loss: 19.8985 - val_mean_absolute_error: 19.8985 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 19.77042\n",
            "Epoch 7/200\n",
            "32000/32000 [==============================] - 4s 130us/step - loss: 19.8097 - mean_absolute_error: 19.8097 - accuracy: 0.0000e+00 - val_loss: 19.7568 - val_mean_absolute_error: 19.7568 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00007: val_loss improved from 19.77042 to 19.75678, saving model to Train 2 Weights-007--19.75678.hdf5\n",
            "Epoch 8/200\n",
            "32000/32000 [==============================] - 4s 130us/step - loss: 19.7465 - mean_absolute_error: 19.7465 - accuracy: 0.0000e+00 - val_loss: 19.7588 - val_mean_absolute_error: 19.7588 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 19.75678\n",
            "Epoch 9/200\n",
            "32000/32000 [==============================] - 4s 131us/step - loss: 19.7543 - mean_absolute_error: 19.7543 - accuracy: 0.0000e+00 - val_loss: 19.7266 - val_mean_absolute_error: 19.7266 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00009: val_loss improved from 19.75678 to 19.72657, saving model to Train 2 Weights-009--19.72657.hdf5\n",
            "Epoch 10/200\n",
            "32000/32000 [==============================] - 4s 134us/step - loss: 19.7388 - mean_absolute_error: 19.7388 - accuracy: 0.0000e+00 - val_loss: 19.8524 - val_mean_absolute_error: 19.8524 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 19.72657\n",
            "Epoch 11/200\n",
            "32000/32000 [==============================] - 4s 138us/step - loss: 19.7467 - mean_absolute_error: 19.7467 - accuracy: 0.0000e+00 - val_loss: 19.8077 - val_mean_absolute_error: 19.8077 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 19.72657\n",
            "Epoch 12/200\n",
            "32000/32000 [==============================] - 4s 137us/step - loss: 19.6862 - mean_absolute_error: 19.6862 - accuracy: 0.0000e+00 - val_loss: 19.7029 - val_mean_absolute_error: 19.7029 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00012: val_loss improved from 19.72657 to 19.70294, saving model to Train 2 Weights-012--19.70294.hdf5\n",
            "Epoch 13/200\n",
            "32000/32000 [==============================] - 4s 131us/step - loss: 19.6941 - mean_absolute_error: 19.6941 - accuracy: 0.0000e+00 - val_loss: 19.7120 - val_mean_absolute_error: 19.7120 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 19.70294\n",
            "Epoch 14/200\n",
            "32000/32000 [==============================] - 4s 137us/step - loss: 19.7170 - mean_absolute_error: 19.7171 - accuracy: 0.0000e+00 - val_loss: 19.9577 - val_mean_absolute_error: 19.9577 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 19.70294\n",
            "Epoch 15/200\n",
            "32000/32000 [==============================] - 4s 132us/step - loss: 19.6561 - mean_absolute_error: 19.6561 - accuracy: 0.0000e+00 - val_loss: 19.6787 - val_mean_absolute_error: 19.6787 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00015: val_loss improved from 19.70294 to 19.67875, saving model to Train 2 Weights-015--19.67875.hdf5\n",
            "Epoch 16/200\n",
            "32000/32000 [==============================] - 4s 139us/step - loss: 19.7095 - mean_absolute_error: 19.7095 - accuracy: 0.0000e+00 - val_loss: 19.7066 - val_mean_absolute_error: 19.7066 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 19.67875\n",
            "Epoch 17/200\n",
            "32000/32000 [==============================] - 4s 137us/step - loss: 19.6242 - mean_absolute_error: 19.6242 - accuracy: 0.0000e+00 - val_loss: 19.7712 - val_mean_absolute_error: 19.7712 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 19.67875\n",
            "Epoch 18/200\n",
            "32000/32000 [==============================] - 4s 136us/step - loss: 19.6385 - mean_absolute_error: 19.6385 - accuracy: 0.0000e+00 - val_loss: 19.8117 - val_mean_absolute_error: 19.8117 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 19.67875\n",
            "Epoch 19/200\n",
            "32000/32000 [==============================] - 4s 133us/step - loss: 19.6282 - mean_absolute_error: 19.6282 - accuracy: 0.0000e+00 - val_loss: 19.7771 - val_mean_absolute_error: 19.7771 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 19.67875\n",
            "Epoch 20/200\n",
            "32000/32000 [==============================] - 4s 133us/step - loss: 19.6206 - mean_absolute_error: 19.6206 - accuracy: 0.0000e+00 - val_loss: 19.6737 - val_mean_absolute_error: 19.6737 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00020: val_loss improved from 19.67875 to 19.67372, saving model to Train 2 Weights-020--19.67372.hdf5\n",
            "Epoch 21/200\n",
            "32000/32000 [==============================] - 4s 133us/step - loss: 19.6163 - mean_absolute_error: 19.6163 - accuracy: 0.0000e+00 - val_loss: 20.0774 - val_mean_absolute_error: 20.0774 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 19.67372\n",
            "Epoch 22/200\n",
            "32000/32000 [==============================] - 4s 131us/step - loss: 19.5964 - mean_absolute_error: 19.5964 - accuracy: 0.0000e+00 - val_loss: 19.8051 - val_mean_absolute_error: 19.8051 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 19.67372\n",
            "Epoch 23/200\n",
            "32000/32000 [==============================] - 5s 142us/step - loss: 19.5779 - mean_absolute_error: 19.5779 - accuracy: 0.0000e+00 - val_loss: 19.5985 - val_mean_absolute_error: 19.5985 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00023: val_loss improved from 19.67372 to 19.59847, saving model to Train 2 Weights-023--19.59847.hdf5\n",
            "Epoch 24/200\n",
            "32000/32000 [==============================] - 4s 134us/step - loss: 19.5737 - mean_absolute_error: 19.5737 - accuracy: 0.0000e+00 - val_loss: 19.5832 - val_mean_absolute_error: 19.5832 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00024: val_loss improved from 19.59847 to 19.58316, saving model to Train 2 Weights-024--19.58316.hdf5\n",
            "Epoch 25/200\n",
            "32000/32000 [==============================] - 4s 129us/step - loss: 19.5471 - mean_absolute_error: 19.5471 - accuracy: 0.0000e+00 - val_loss: 19.8545 - val_mean_absolute_error: 19.8545 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 19.58316\n",
            "Epoch 26/200\n",
            "32000/32000 [==============================] - 4s 131us/step - loss: 19.5044 - mean_absolute_error: 19.5044 - accuracy: 0.0000e+00 - val_loss: 19.5650 - val_mean_absolute_error: 19.5650 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00026: val_loss improved from 19.58316 to 19.56498, saving model to Train 2 Weights-026--19.56498.hdf5\n",
            "Epoch 27/200\n",
            "32000/32000 [==============================] - 4s 131us/step - loss: 19.5032 - mean_absolute_error: 19.5032 - accuracy: 0.0000e+00 - val_loss: 19.5077 - val_mean_absolute_error: 19.5077 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00027: val_loss improved from 19.56498 to 19.50767, saving model to Train 2 Weights-027--19.50767.hdf5\n",
            "Epoch 28/200\n",
            "32000/32000 [==============================] - 4s 128us/step - loss: 19.4445 - mean_absolute_error: 19.4445 - accuracy: 0.0000e+00 - val_loss: 19.5124 - val_mean_absolute_error: 19.5124 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 19.50767\n",
            "Epoch 29/200\n",
            "32000/32000 [==============================] - 4s 129us/step - loss: 19.4441 - mean_absolute_error: 19.4441 - accuracy: 0.0000e+00 - val_loss: 19.4305 - val_mean_absolute_error: 19.4305 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00029: val_loss improved from 19.50767 to 19.43052, saving model to Train 2 Weights-029--19.43052.hdf5\n",
            "Epoch 30/200\n",
            "32000/32000 [==============================] - 5s 144us/step - loss: 19.3896 - mean_absolute_error: 19.3895 - accuracy: 0.0000e+00 - val_loss: 19.4301 - val_mean_absolute_error: 19.4300 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00030: val_loss improved from 19.43052 to 19.43006, saving model to Train 2 Weights-030--19.43006.hdf5\n",
            "Epoch 31/200\n",
            "32000/32000 [==============================] - 5s 146us/step - loss: 19.3686 - mean_absolute_error: 19.3686 - accuracy: 0.0000e+00 - val_loss: 19.4214 - val_mean_absolute_error: 19.4214 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00031: val_loss improved from 19.43006 to 19.42145, saving model to Train 2 Weights-031--19.42145.hdf5\n",
            "Epoch 32/200\n",
            "32000/32000 [==============================] - 5s 142us/step - loss: 19.3618 - mean_absolute_error: 19.3618 - accuracy: 0.0000e+00 - val_loss: 19.4154 - val_mean_absolute_error: 19.4154 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00032: val_loss improved from 19.42145 to 19.41542, saving model to Train 2 Weights-032--19.41542.hdf5\n",
            "Epoch 33/200\n",
            "32000/32000 [==============================] - 4s 136us/step - loss: 19.3553 - mean_absolute_error: 19.3553 - accuracy: 0.0000e+00 - val_loss: 19.5109 - val_mean_absolute_error: 19.5109 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 19.41542\n",
            "Epoch 34/200\n",
            "32000/32000 [==============================] - 4s 135us/step - loss: 19.3504 - mean_absolute_error: 19.3504 - accuracy: 0.0000e+00 - val_loss: 19.4674 - val_mean_absolute_error: 19.4674 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 19.41542\n",
            "Epoch 35/200\n",
            "32000/32000 [==============================] - 4s 135us/step - loss: 19.3407 - mean_absolute_error: 19.3407 - accuracy: 0.0000e+00 - val_loss: 19.4102 - val_mean_absolute_error: 19.4102 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00035: val_loss improved from 19.41542 to 19.41019, saving model to Train 2 Weights-035--19.41019.hdf5\n",
            "Epoch 36/200\n",
            "32000/32000 [==============================] - 4s 132us/step - loss: 19.3355 - mean_absolute_error: 19.3355 - accuracy: 0.0000e+00 - val_loss: 19.4190 - val_mean_absolute_error: 19.4190 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 19.41019\n",
            "Epoch 37/200\n",
            "32000/32000 [==============================] - 4s 139us/step - loss: 19.3411 - mean_absolute_error: 19.3411 - accuracy: 0.0000e+00 - val_loss: 19.4855 - val_mean_absolute_error: 19.4855 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 19.41019\n",
            "Epoch 38/200\n",
            "32000/32000 [==============================] - 4s 140us/step - loss: 19.3512 - mean_absolute_error: 19.3512 - accuracy: 0.0000e+00 - val_loss: 19.4064 - val_mean_absolute_error: 19.4064 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00038: val_loss improved from 19.41019 to 19.40641, saving model to Train 2 Weights-038--19.40641.hdf5\n",
            "Epoch 39/200\n",
            "32000/32000 [==============================] - 4s 131us/step - loss: 19.3308 - mean_absolute_error: 19.3308 - accuracy: 0.0000e+00 - val_loss: 19.4720 - val_mean_absolute_error: 19.4720 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 19.40641\n",
            "Epoch 40/200\n",
            "32000/32000 [==============================] - 4s 130us/step - loss: 19.3518 - mean_absolute_error: 19.3518 - accuracy: 0.0000e+00 - val_loss: 19.4458 - val_mean_absolute_error: 19.4458 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 19.40641\n",
            "Epoch 41/200\n",
            "32000/32000 [==============================] - 4s 134us/step - loss: 19.3201 - mean_absolute_error: 19.3201 - accuracy: 0.0000e+00 - val_loss: 19.4055 - val_mean_absolute_error: 19.4055 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00041: val_loss improved from 19.40641 to 19.40547, saving model to Train 2 Weights-041--19.40547.hdf5\n",
            "Epoch 42/200\n",
            "32000/32000 [==============================] - 4s 131us/step - loss: 19.3351 - mean_absolute_error: 19.3351 - accuracy: 0.0000e+00 - val_loss: 19.4027 - val_mean_absolute_error: 19.4027 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00042: val_loss improved from 19.40547 to 19.40268, saving model to Train 2 Weights-042--19.40268.hdf5\n",
            "Epoch 43/200\n",
            "32000/32000 [==============================] - 4s 137us/step - loss: 19.3374 - mean_absolute_error: 19.3374 - accuracy: 0.0000e+00 - val_loss: 19.4823 - val_mean_absolute_error: 19.4824 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 19.40268\n",
            "Epoch 44/200\n",
            "32000/32000 [==============================] - 4s 135us/step - loss: 19.3432 - mean_absolute_error: 19.3431 - accuracy: 0.0000e+00 - val_loss: 19.4037 - val_mean_absolute_error: 19.4037 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 19.40268\n",
            "Epoch 45/200\n",
            "32000/32000 [==============================] - 4s 129us/step - loss: 19.3330 - mean_absolute_error: 19.3330 - accuracy: 0.0000e+00 - val_loss: 19.4321 - val_mean_absolute_error: 19.4321 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 19.40268\n",
            "Epoch 46/200\n",
            "32000/32000 [==============================] - 4s 130us/step - loss: 19.3471 - mean_absolute_error: 19.3471 - accuracy: 0.0000e+00 - val_loss: 19.4659 - val_mean_absolute_error: 19.4659 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 19.40268\n",
            "Epoch 47/200\n",
            "32000/32000 [==============================] - 4s 132us/step - loss: 19.3423 - mean_absolute_error: 19.3423 - accuracy: 0.0000e+00 - val_loss: 19.5182 - val_mean_absolute_error: 19.5182 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 19.40268\n",
            "Epoch 48/200\n",
            "32000/32000 [==============================] - 4s 125us/step - loss: 19.3408 - mean_absolute_error: 19.3409 - accuracy: 0.0000e+00 - val_loss: 19.4066 - val_mean_absolute_error: 19.4066 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 19.40268\n",
            "Epoch 49/200\n",
            "32000/32000 [==============================] - 4s 131us/step - loss: 19.3383 - mean_absolute_error: 19.3383 - accuracy: 0.0000e+00 - val_loss: 19.8218 - val_mean_absolute_error: 19.8218 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 19.40268\n",
            "Epoch 50/200\n",
            "32000/32000 [==============================] - 4s 128us/step - loss: 19.3410 - mean_absolute_error: 19.3410 - accuracy: 0.0000e+00 - val_loss: 19.4047 - val_mean_absolute_error: 19.4047 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 19.40268\n",
            "Epoch 51/200\n",
            "32000/32000 [==============================] - 4s 135us/step - loss: 19.3260 - mean_absolute_error: 19.3260 - accuracy: 0.0000e+00 - val_loss: 19.4033 - val_mean_absolute_error: 19.4033 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 19.40268\n",
            "Epoch 52/200\n",
            "32000/32000 [==============================] - 4s 133us/step - loss: 19.3275 - mean_absolute_error: 19.3274 - accuracy: 0.0000e+00 - val_loss: 19.5583 - val_mean_absolute_error: 19.5583 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 19.40268\n",
            "Epoch 53/200\n",
            "32000/32000 [==============================] - 4s 130us/step - loss: 19.3335 - mean_absolute_error: 19.3335 - accuracy: 0.0000e+00 - val_loss: 19.4163 - val_mean_absolute_error: 19.4163 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 19.40268\n",
            "Epoch 54/200\n",
            "32000/32000 [==============================] - 4s 132us/step - loss: 19.3502 - mean_absolute_error: 19.3502 - accuracy: 0.0000e+00 - val_loss: 19.5019 - val_mean_absolute_error: 19.5019 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 19.40268\n",
            "Epoch 55/200\n",
            "32000/32000 [==============================] - 4s 131us/step - loss: 19.3251 - mean_absolute_error: 19.3251 - accuracy: 0.0000e+00 - val_loss: 19.9032 - val_mean_absolute_error: 19.9032 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 19.40268\n",
            "Epoch 56/200\n",
            "32000/32000 [==============================] - 4s 134us/step - loss: 19.3397 - mean_absolute_error: 19.3397 - accuracy: 0.0000e+00 - val_loss: 19.4787 - val_mean_absolute_error: 19.4787 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 19.40268\n",
            "Epoch 57/200\n",
            "32000/32000 [==============================] - 4s 131us/step - loss: 19.3316 - mean_absolute_error: 19.3316 - accuracy: 0.0000e+00 - val_loss: 19.4414 - val_mean_absolute_error: 19.4414 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 19.40268\n",
            "Epoch 58/200\n",
            "32000/32000 [==============================] - 4s 132us/step - loss: 19.3252 - mean_absolute_error: 19.3252 - accuracy: 0.0000e+00 - val_loss: 19.4384 - val_mean_absolute_error: 19.4384 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 19.40268\n",
            "Epoch 59/200\n",
            "32000/32000 [==============================] - 4s 135us/step - loss: 19.3412 - mean_absolute_error: 19.3412 - accuracy: 0.0000e+00 - val_loss: 19.4402 - val_mean_absolute_error: 19.4402 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 19.40268\n",
            "Epoch 60/200\n",
            "32000/32000 [==============================] - 4s 131us/step - loss: 19.3451 - mean_absolute_error: 19.3451 - accuracy: 0.0000e+00 - val_loss: 19.4062 - val_mean_absolute_error: 19.4062 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 19.40268\n",
            "Epoch 61/200\n",
            "32000/32000 [==============================] - 4s 137us/step - loss: 19.3365 - mean_absolute_error: 19.3365 - accuracy: 0.0000e+00 - val_loss: 19.4144 - val_mean_absolute_error: 19.4144 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 19.40268\n",
            "Epoch 62/200\n",
            "32000/32000 [==============================] - 4s 132us/step - loss: 19.3370 - mean_absolute_error: 19.3370 - accuracy: 0.0000e+00 - val_loss: 19.4037 - val_mean_absolute_error: 19.4037 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 19.40268\n",
            "Epoch 63/200\n",
            "32000/32000 [==============================] - 4s 133us/step - loss: 19.3276 - mean_absolute_error: 19.3276 - accuracy: 0.0000e+00 - val_loss: 19.4105 - val_mean_absolute_error: 19.4105 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 19.40268\n",
            "Epoch 64/200\n",
            "32000/32000 [==============================] - 4s 127us/step - loss: 19.3192 - mean_absolute_error: 19.3192 - accuracy: 0.0000e+00 - val_loss: 19.4029 - val_mean_absolute_error: 19.4029 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 19.40268\n",
            "Epoch 65/200\n",
            "32000/32000 [==============================] - 4s 132us/step - loss: 19.3378 - mean_absolute_error: 19.3378 - accuracy: 0.0000e+00 - val_loss: 19.4398 - val_mean_absolute_error: 19.4398 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 19.40268\n",
            "Epoch 66/200\n",
            "32000/32000 [==============================] - 4s 131us/step - loss: 19.3352 - mean_absolute_error: 19.3352 - accuracy: 0.0000e+00 - val_loss: 19.6319 - val_mean_absolute_error: 19.6319 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 19.40268\n",
            "Epoch 67/200\n",
            "32000/32000 [==============================] - 4s 133us/step - loss: 19.3276 - mean_absolute_error: 19.3276 - accuracy: 0.0000e+00 - val_loss: 19.4329 - val_mean_absolute_error: 19.4329 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 19.40268\n",
            "Epoch 68/200\n",
            "32000/32000 [==============================] - 4s 134us/step - loss: 19.3255 - mean_absolute_error: 19.3255 - accuracy: 0.0000e+00 - val_loss: 19.4783 - val_mean_absolute_error: 19.4783 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 19.40268\n",
            "Epoch 69/200\n",
            "32000/32000 [==============================] - 4s 133us/step - loss: 19.3260 - mean_absolute_error: 19.3260 - accuracy: 0.0000e+00 - val_loss: 19.4230 - val_mean_absolute_error: 19.4230 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 19.40268\n",
            "Epoch 70/200\n",
            "32000/32000 [==============================] - 4s 137us/step - loss: 19.3277 - mean_absolute_error: 19.3277 - accuracy: 0.0000e+00 - val_loss: 19.4467 - val_mean_absolute_error: 19.4467 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 19.40268\n",
            "Epoch 71/200\n",
            "32000/32000 [==============================] - 5s 143us/step - loss: 19.3319 - mean_absolute_error: 19.3319 - accuracy: 0.0000e+00 - val_loss: 19.4130 - val_mean_absolute_error: 19.4130 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 19.40268\n",
            "Epoch 72/200\n",
            "32000/32000 [==============================] - 4s 130us/step - loss: 19.3285 - mean_absolute_error: 19.3285 - accuracy: 0.0000e+00 - val_loss: 19.4803 - val_mean_absolute_error: 19.4803 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 19.40268\n",
            "Epoch 73/200\n",
            "32000/32000 [==============================] - 4s 132us/step - loss: 19.3323 - mean_absolute_error: 19.3323 - accuracy: 0.0000e+00 - val_loss: 19.4036 - val_mean_absolute_error: 19.4036 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 19.40268\n",
            "Epoch 74/200\n",
            "32000/32000 [==============================] - 4s 134us/step - loss: 19.3352 - mean_absolute_error: 19.3352 - accuracy: 0.0000e+00 - val_loss: 19.4231 - val_mean_absolute_error: 19.4231 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 19.40268\n",
            "Epoch 75/200\n",
            "32000/32000 [==============================] - 4s 130us/step - loss: 19.3201 - mean_absolute_error: 19.3201 - accuracy: 0.0000e+00 - val_loss: 19.5765 - val_mean_absolute_error: 19.5765 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 19.40268\n",
            "Epoch 76/200\n",
            "32000/32000 [==============================] - 4s 129us/step - loss: 19.3186 - mean_absolute_error: 19.3186 - accuracy: 0.0000e+00 - val_loss: 19.4103 - val_mean_absolute_error: 19.4103 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 19.40268\n",
            "Epoch 77/200\n",
            "32000/32000 [==============================] - 4s 132us/step - loss: 19.3221 - mean_absolute_error: 19.3222 - accuracy: 0.0000e+00 - val_loss: 19.6653 - val_mean_absolute_error: 19.6653 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 19.40268\n",
            "Epoch 78/200\n",
            "32000/32000 [==============================] - 4s 125us/step - loss: 19.3260 - mean_absolute_error: 19.3260 - accuracy: 0.0000e+00 - val_loss: 19.4425 - val_mean_absolute_error: 19.4425 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 19.40268\n",
            "Epoch 79/200\n",
            "32000/32000 [==============================] - 4s 134us/step - loss: 19.3228 - mean_absolute_error: 19.3228 - accuracy: 0.0000e+00 - val_loss: 19.4657 - val_mean_absolute_error: 19.4657 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 19.40268\n",
            "Epoch 80/200\n",
            "32000/32000 [==============================] - 4s 131us/step - loss: 19.3189 - mean_absolute_error: 19.3189 - accuracy: 0.0000e+00 - val_loss: 19.4180 - val_mean_absolute_error: 19.4180 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 19.40268\n",
            "Epoch 81/200\n",
            "32000/32000 [==============================] - 4s 133us/step - loss: 19.3256 - mean_absolute_error: 19.3256 - accuracy: 0.0000e+00 - val_loss: 19.4389 - val_mean_absolute_error: 19.4389 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 19.40268\n",
            "Epoch 82/200\n",
            "32000/32000 [==============================] - 4s 131us/step - loss: 19.3202 - mean_absolute_error: 19.3202 - accuracy: 0.0000e+00 - val_loss: 19.4972 - val_mean_absolute_error: 19.4972 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 19.40268\n",
            "Epoch 83/200\n",
            "32000/32000 [==============================] - 4s 130us/step - loss: 19.3156 - mean_absolute_error: 19.3156 - accuracy: 0.0000e+00 - val_loss: 19.5221 - val_mean_absolute_error: 19.5221 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 19.40268\n",
            "Epoch 84/200\n",
            "32000/32000 [==============================] - 4s 134us/step - loss: 19.3310 - mean_absolute_error: 19.3310 - accuracy: 0.0000e+00 - val_loss: 19.4230 - val_mean_absolute_error: 19.4230 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 19.40268\n",
            "Epoch 85/200\n",
            "32000/32000 [==============================] - 4s 132us/step - loss: 19.3167 - mean_absolute_error: 19.3167 - accuracy: 0.0000e+00 - val_loss: 19.5182 - val_mean_absolute_error: 19.5182 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 19.40268\n",
            "Epoch 86/200\n",
            "32000/32000 [==============================] - 4s 130us/step - loss: 19.3266 - mean_absolute_error: 19.3266 - accuracy: 0.0000e+00 - val_loss: 19.4113 - val_mean_absolute_error: 19.4113 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 19.40268\n",
            "Epoch 87/200\n",
            "28672/32000 [=========================>....] - ETA: 0s - loss: 19.3240 - mean_absolute_error: 19.3240 - accuracy: 0.0000e+00"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-89934511da4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcallbacks_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_ZcMZNp0pjM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "fc34e59a-bbc9-480d-83bb-fc6669d0d976"
      },
      "source": [
        "import keras\n",
        "from matplotlib import pyplot as plt\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCrQtjuYKo1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  NN_model = Sequential()\n",
        "\n",
        "  # The Input Layer :\n",
        "  NN_model.add(Dense(128, kernel_initializer='normal',input_dim=7, activation='relu'))\n",
        "\n",
        "  # The Hidden Layers :\n",
        "  NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "  NN_model.add(Dense(128, kernel_initializer='normal',activation='relu'))\n",
        "  NN_model.add(Dense(64, kernel_initializer='normal',activation='relu'))\n",
        "\n",
        "  # The Output Layer :\n",
        "  NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
        "\n",
        "  # Compile the network :\n",
        "  NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "  NN_model.summary()\n",
        "  return NN_model"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFCK7xbY18Vz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "4276295b-a7c1-4d54-e82d-61089deec7db"
      },
      "source": [
        "import keras\n",
        "from keras.models import load_model\n",
        "\n",
        "model = build_model()\n",
        "model.load_weights('Weights-158--0.01271.hdf5')"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_76 (Dense)             (None, 128)               1024      \n",
            "_________________________________________________________________\n",
            "dense_77 (Dense)             (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dense_78 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_79 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_80 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 75,265\n",
            "Trainable params: 75,265\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5CVM0NyKdBX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "f56a1fc6-0211-4aa2-d5cd-9b6b6869e1d6"
      },
      "source": [
        "import numpy as np\n",
        "test = train[3999:]\n",
        "model.predict(test)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[48.953106],\n",
              "       [51.63001 ],\n",
              "       [33.673058],\n",
              "       ...,\n",
              "       [35.086708],\n",
              "       [60.78499 ],\n",
              "       [57.941093]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsgO8TSzLXgm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYDYUwt5M4xZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}